{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e55496-34a9-432b-a7d5-afdc9da07118",
   "metadata": {},
   "source": [
    "# Live Inference on WikiMedia Data\n",
    "\n",
    "This notebook illustrates a simple ML pipeline that leverages `csp` for feature generation and real-time (i.e. live) inference. \n",
    "\n",
    "The data we are going to leverage is the [MediaWiki Recent Changes feed](https://www.mediawiki.org/wiki/Manual:RCFeed) stream, which emits events related to recent changes across Wikimedia sites. The stream can be accessed through the [EventStreams web service](https://wikitech.wikimedia.org/wiki/Event_Platform/EventStreams_HTTP_Service). \n",
    "Our objective will be to predict whether a change is being made by a bot account, without leveraging the the `bot` indicator on the events or the account name (as bot accounts typically are good citizens and have the word \"bot\" in their name). \n",
    "\n",
    "The high-level outline of this notebook is as follows\n",
    " * **Historical Data Collection**: Pull historical data from the stream, which will give us the ground truth (the \"bot\") flag and some metadata for generating features\n",
    " * **Feature Generation**: Write some simple feature generation logic using `csp`, and run it on the historical data to generate a feature set\n",
    " * **Model Training**: Train a classification model on the data with `sklearn`\n",
    " * **Real Time Data Adapter**: Write a real time data adapter for `csp` (see the [csp wikimedia example](https://github.com/Point72/csp/blob/main/examples/07_end_to_end/wikimedia.ipynb) for a more in-depth explanation)\n",
    " * **Real Time Inference and Monitoring**: Write simple real-time inference and monitoring nodes with `csp`\n",
    " * **Run Real Time Inference!**\n",
    "\n",
    "To run this notebook, we need to install a few extra dependencies, listed below:\n",
    "```\n",
    "pip install sseclient\n",
    "pip install perspective-python[jupyter]\n",
    "pip install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f580d-64a5-44a6-9226-cae7f42c3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import threading\n",
    "from datetime import date, datetime, timedelta\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "from perspective.widget import PerspectiveWidget\n",
    "from sseclient import SSEClient as EventSource\n",
    "\n",
    "import csp\n",
    "from csp import Outputs, ts\n",
    "from csp.typing import NumpyNDArray\n",
    "from csp.utils.datetime import utc_now\n",
    "\n",
    "np.set_printoptions(edgeitems=5, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c105b-b70f-427c-aa38-ad7969873b1d",
   "metadata": {},
   "source": [
    "## Historical Data Collection\n",
    "\n",
    "In this stage, we pull a fixed number of events from the recent change feed dating back several days. This historical data will be used to get the groud-truth (the labels for our model) as well as the raw data we will use for feature generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da6ddb8-b18b-48d4-b23e-c1daaf1ae293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "55776 records\n",
      "CPU times: user 13 s, sys: 1.86 s, total: 14.8 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time  \n",
    "# Takes about a minute\n",
    "\n",
    "# We pull the most recent data because older data is purged from the feed\n",
    "since = date.today() - timedelta(days=7)\n",
    "URL = f\"https://stream.wikimedia.org/v2/stream/recentchange?since={since}\"\n",
    "N = 100_000  # Maximum number of events to pull\n",
    "data = []\n",
    "for idx, item in enumerate(EventSource(URL)):\n",
    "    if item.event == \"message\":\n",
    "        try:\n",
    "            change = json.loads(item.data)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        # discard canary events\n",
    "        if change[\"meta\"][\"domain\"] == \"canary\" or change[\"type\"] not in (\"new\", \"edit\"):\n",
    "            continue\n",
    "        data.append(change)\n",
    "        if idx >= N:\n",
    "            break\n",
    "data = sorted(data, key=lambda d: d[\"timestamp\"])\n",
    "print(f\"{len(data)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985541b-809f-4477-9a8a-131010913643",
   "metadata": {},
   "source": [
    "Below is what a sample event looks like. Take note of the `bot` key, which will tell us whether a given event was created by a bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9931584-791a-4cb3-bd95-6c56b838fd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$schema': '/mediawiki/recentchange/1.0.0',\n",
       " 'meta': {'uri': 'https://en.wikipedia.org/wiki/Wikipedia:Equity_lists/Nationality/Malawi',\n",
       "  'request_id': 'e068d678-ac33-41a4-a049-f365a287576d',\n",
       "  'id': 'd57753bd-6b42-4f09-8a19-96f5f3d8c558',\n",
       "  'dt': '2024-11-25T10:12:24Z',\n",
       "  'domain': 'en.wikipedia.org',\n",
       "  'stream': 'mediawiki.recentchange',\n",
       "  'topic': 'codfw.mediawiki.recentchange',\n",
       "  'partition': 0,\n",
       "  'offset': 1274922687},\n",
       " 'id': 1844881588,\n",
       " 'type': 'edit',\n",
       " 'namespace': 4,\n",
       " 'title': 'Wikipedia:Equity lists/Nationality/Malawi',\n",
       " 'title_url': 'https://en.wikipedia.org/wiki/Wikipedia:Equity_lists/Nationality/Malawi',\n",
       " 'comment': 'Wikidata list updated [V2]',\n",
       " 'timestamp': 1732529544,\n",
       " 'user': 'ListeriaBot',\n",
       " 'bot': True,\n",
       " 'notify_url': 'https://en.wikipedia.org/w/index.php?diff=1259472355&oldid=1258389456',\n",
       " 'minor': False,\n",
       " 'length': {'old': 1067640, 'new': 1118836},\n",
       " 'revision': {'old': 1258389456, 'new': 1259472355},\n",
       " 'server_url': 'https://en.wikipedia.org',\n",
       " 'server_name': 'en.wikipedia.org',\n",
       " 'server_script_path': '/w',\n",
       " 'wiki': 'enwiki',\n",
       " 'parsedcomment': 'Wikidata list updated [V2]'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eef6a6-e945-41c0-9e3d-f7153b3c975d",
   "metadata": {},
   "source": [
    "## Feature Generation\n",
    "\n",
    "The next step of the pipeline is to generate features. \n",
    "\n",
    "Here we use a simple python function to generate a dictionary of features from the event dictionary, including a one-hot encoding of the `server_name` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d60fd7a-0b56-42cf-a48a-8f08645b6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_from_event(event: Dict[str, Any], server_map: Dict[str, str]) -> Dict[str, Any]:\n",
    "    features = {}\n",
    "    # Feature engineering - can do in pandas, but also wrap in csp\n",
    "    features[\"length.new\"] = event[\"length\"][\"new\"]\n",
    "    features[\"length.delta\"] = event[\"length\"][\"new\"] - event[\"length\"].get(\"old\", 0)\n",
    "    features[\"comment.len\"] = len(event[\"comment\"])\n",
    "    # See https://www.mediawiki.org/wiki/Manual:Namespace\n",
    "    features[\"namespace.main\"] = int(event[\"namespace\"] == 0)\n",
    "    # features[\"namespace.talk\"] = int(event[\"namespace\"]) % 2\n",
    "    # features[\"timestamp.hour\"] = datetime.utcfromtimestamp(event[\"timestamp\"]).hour\n",
    "    # features[\"minor\"] = int(event[\"minor\"])\n",
    "\n",
    "    # One hot encoding of specific recognized domains (default is zero for all)\n",
    "    # features[\"server.main\"] = int(event[\"server_name\"] in server_map)\n",
    "    for k, v in server_map.items():\n",
    "        features[f\"server_{v}\"] = int(event[\"server_name\"] == k)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea130a-ca25-4d69-97ba-4ea0ce65bd60",
   "metadata": {},
   "source": [
    "We run the feature generation function on a sample event to illustrate the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3264b1ef-0b8c-46d8-8699-35005f172de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'length.new': 1118836,\n",
       " 'length.delta': 51196,\n",
       " 'comment.len': 26,\n",
       " 'namespace.main': 0,\n",
       " 'server_wikidata': 0,\n",
       " 'server_wikimedia': 0,\n",
       " 'server_wikipedia': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_map = {\"www.wikidata.org\": \"wikidata\", \"commons.wikimedia.org\": \"wikimedia\", \"en.wikipedia.org\": \"wikipedia\"}\n",
    "generate_features_from_event(data[0], server_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be883e6-d0bc-4f43-b92b-c710109e5015",
   "metadata": {},
   "source": [
    "Now we wrap the `generate_features_from_event` python function in a `csp` node that also converts the feature dictionary to a numpy array (which we will pass to `sklearn`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2e6a56-9d44-4121-9aff-d205f020c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@csp.node\n",
    "def generate_features(event: ts[dict], server_map: dict) -> ts[NumpyNDArray[float]]:\n",
    "    features = generate_features_from_event(event, server_map)\n",
    "    return np.array(list(features.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390cc28d-d3fa-43e1-9806-d1c5d8f0053a",
   "metadata": {},
   "source": [
    "In this simple example, the feature generation is represented by a single node, and the features are a function of the current event only, but with `csp`, it would be possible to build a feature graph consisting of multiple nodes, and where the features depended on state built from past events (i.e. recent events counts, etc).\n",
    "\n",
    "With the tools above in place, we can now generate a set of historical features with csp. We pass the historical events to csp using `csp.curve` - associating each event with a timestamp for the csp engine. \n",
    "By calling `csp.run` on the `generate_features` node, we produce two arrays - one of the output timestamps and one of the feature arrays. To get a 2-d array of features, we stack the output features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a368e957-f5a0-4606-9cab-9c4ebdac4b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55776, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1118836,   51196,      26,       0,       0,       0,       1],\n",
       "       [ 463624,      14,      27,       0,       0,       0,       0],\n",
       "       [ 306317,     159,      94,       0,       0,       0,       1],\n",
       "       [  64988,       0,      19,       1,       0,       0,       1],\n",
       "       [ 207533,    -159,     135,       1,       0,       0,       1],\n",
       "       ...,\n",
       "       [  12902,       3,     127,       1,       1,       0,       0],\n",
       "       [  20075,       3,     123,       1,       1,       0,       0],\n",
       "       [  20078,       3,     126,       1,       1,       0,       0],\n",
       "       [  85265,     288,     132,       1,       1,       0,       0],\n",
       "       [  14524,      -1,      58,       1,       1,       0,       0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_input = csp.curve(dict, [(datetime.utcfromtimestamp(event[\"timestamp\"]), event) for event in data])\n",
    "times, features = csp.run(generate_features, hist_input, server_map, starttime=datetime(2020, 1, 1), output_numpy=True)[\n",
    "    0\n",
    "]\n",
    "X = np.vstack(features)\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4dfc51-7c5c-43f9-8522-48449c076a50",
   "metadata": {},
   "source": [
    "To generate the labels, we simply pull the `bot` field from each event and put the results in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "632b9b90-5317-4ec5-a8ec-b66c13302c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([int(event[\"bot\"]) for event in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0dff4-9acd-4d01-b95c-bd153ce85579",
   "metadata": {},
   "source": [
    "The reason that we don't use this strategy for the feature generation is because we want to be sure that we are using the same feature generation logic in real time (where we are running a csp graph) as we do in simulation. \n",
    "\n",
    "As feature graphs get more complex and as they depend on stateful features or multiple asynchronous feeds, this becomes more important than in this simple example in which features are just a function of each event coming from a single source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ba81d-8e77-4614-b3ab-5aadc07e0349",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "To train a model on the generated features, we use the `RandomForestClassifier` from `sklearn` with some parameters chosen from their [Classifier Comparison](https://scikit-learn.org/1.5/auto_examples/classification/plot_classifier_comparison.html). The model appears to do well enough on the historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48f13fe3-5bb1-447f-8d0e-25cd569572bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9386401326699834\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      9506\n",
      "           1       0.98      0.91      0.94     12805\n",
      "\n",
      "    accuracy                           0.94     22311\n",
      "   macro avg       0.94      0.94      0.94     22311\n",
      "weighted avg       0.94      0.94      0.94     22311\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 9250   256]\n",
      " [ 1113 11692]]\n",
      "CPU times: user 348 ms, sys: 47.6 ms, total: 396 ms\n",
      "Wall time: 612 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, shuffle=False)\n",
    "\n",
    "clf0 = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=42)\n",
    "clf = make_pipeline(StandardScaler(), clf0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Compute the score\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Accuracy: \", score)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f73704-7a88-45b3-8e2a-84fdc21e85c7",
   "metadata": {},
   "source": [
    "Now we have a trained model, `clf` that we will plug into live inference later on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d9a4a-e903-450f-b142-f50686239cad",
   "metadata": {},
   "source": [
    "## Real Time Data Adapter\n",
    "\n",
    "Below is a real time `csp` adapter to stream the data. For a more in-depth overview of how this adapter works, refer to the [csp wikimedia example](https://github.com/Point72/csp/blob/main/examples/07_end_to_end/wikimedia.ipynb).\n",
    "The main difference in the adapter below is that we publish each event as a `dict` (instead of a `csp.Struct`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae0f7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csp.impl.pushadapter import PushInputAdapter\n",
    "from csp.impl.wiring import py_push_adapter_def\n",
    "\n",
    "\n",
    "# Define the runtime implementation of our adapter\n",
    "class FetchWikiDataAdapter(PushInputAdapter):\n",
    "    def __init__(self, url: str):\n",
    "        self._thread = None\n",
    "        self._running = False\n",
    "        self._url = url\n",
    "\n",
    "    def start(self, starttime, endtime):\n",
    "        print(\"FetchWikiDataAdapter::start\")\n",
    "        self.endtime = endtime\n",
    "        self.starttime = starttime\n",
    "        self._source = EventSource(self._url)\n",
    "        self._running = True\n",
    "        self._thread = threading.Thread(target=self._run)\n",
    "        self._thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        print(\"FetchWikiDataAdapter::stop\")\n",
    "        if self._running:\n",
    "            self._running = False\n",
    "            self._thread.join()\n",
    "            self._source.resp.close()\n",
    "\n",
    "    def _run(self):\n",
    "        for item in self._source:\n",
    "            if not self._running:\n",
    "                break\n",
    "            if item.event == \"message\":\n",
    "                try:\n",
    "                    change = json.loads(item.data)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                else:\n",
    "                    # discard canary events\n",
    "                    if change[\"meta\"][\"domain\"] == \"canary\" or change[\"type\"] not in (\"new\", \"edit\"):\n",
    "                        continue\n",
    "\n",
    "                    self.push_tick(change)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "# Create the graph-time representation of our adapter\n",
    "FetchWikiData = py_push_adapter_def(\"FetchWikiData\", FetchWikiDataAdapter, csp.ts[dict], url=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf482d-f578-4726-bca7-81487499dcf0",
   "metadata": {},
   "source": [
    "## Real Time Inference and Monitoring\n",
    "\n",
    "The inference node (which can be used either in real-time or historically) simply takes a model and a stream of feature arrays, and calls predict on the arrays. It outputs the result on the `predictions` output edge in the graph. If there are any exceptions, it outputs the message on the `errors` edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72076247-cce7-4549-ac7e-8b3c7e51c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@csp.node\n",
    "def inference_node(model: object, features: ts[NumpyNDArray[float]]) -> Outputs(predictions=ts[bool], errors=ts[str]):\n",
    "    try:\n",
    "        pred = model.predict(features.reshape(1, -1))\n",
    "        csp.output(predictions=bool(pred[0]))\n",
    "    except Exception as e:\n",
    "        csp.output(errors=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf73e21-ae7c-4ecf-8cdf-4b5bf61cb6f3",
   "metadata": {},
   "source": [
    "Next, we write a node that will update a Perspective Widget so that we can track performance in real time. Note that we buffer the events to limit the rate of UI updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b43196d-f9ad-4e9b-a9a6-2c3a88859d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@csp.node\n",
    "def update_widget(\n",
    "    event: ts[dict], prediction: ts[bool], widget: PerspectiveWidget, throttle: timedelta = timedelta(seconds=0.5)\n",
    "):\n",
    "    # Updates the perspective widget with batched updates for scalability\n",
    "    with csp.alarms():\n",
    "        alarm = csp.alarm(bool)\n",
    "\n",
    "    with csp.state():\n",
    "        s_buffer = []\n",
    "\n",
    "    with csp.start():\n",
    "        csp.schedule_alarm(alarm, throttle, True)\n",
    "\n",
    "    if csp.ticked(event, prediction):\n",
    "        s_buffer.append(\n",
    "            {\n",
    "                \"prediction\": prediction,\n",
    "                \"bot\": event[\"bot\"],\n",
    "                \"timestamp\": event[\"timestamp\"] * 1e3,\n",
    "                \"user\": event.get(\"user\"),\n",
    "                \"title\": event.get(\"title\"),\n",
    "                \"domain\": event[\"meta\"][\"domain\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if csp.ticked(alarm):\n",
    "        if len(s_buffer) > 0:\n",
    "            widget.update(s_buffer)\n",
    "            s_buffer = []\n",
    "\n",
    "        csp.schedule_alarm(alarm, throttle, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef83b4-d9aa-47d5-bb4b-6c4b30ee24b8",
   "metadata": {},
   "source": [
    "Lastly, we put it all together in the inference graph. This graph takes a model and a widget, and then \n",
    "  * generates features\n",
    "  * performs inference\n",
    "  * publishes the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c7ebcb-2a64-4d3f-bef5-28e9c5a33079",
   "metadata": {},
   "outputs": [],
   "source": [
    "@csp.graph\n",
    "def inference_graph(model: object, widget: Optional[PerspectiveWidget]):\n",
    "    server_map = {\"www.wikidata.org\": \"wikidata\", \"commons.wikimedia.org\": \"wikimedia\", \"en.wikipedia.org\": \"wikipedia\"}\n",
    "    raw_input = FetchWikiData(url=\"https://stream.wikimedia.org/v2/stream/recentchange\")\n",
    "    features = generate_features(raw_input, server_map)\n",
    "    outputs = inference_node(model, features)\n",
    "    if widget:\n",
    "        update_widget(raw_input, outputs.predictions, widget)\n",
    "    else:  # In case you want to run without perspective, pass widget = None\n",
    "        csp.print(\"Predictions\", outputs.predictions)\n",
    "        csp.print(\"Target\", csp.apply(raw_input, lambda d: d[\"bot\"], bool))\n",
    "    csp.print(\"Errors\", outputs.errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83492497-b947-42ba-8b1f-cc5335c582a9",
   "metadata": {},
   "source": [
    "## Run Real-Time Inference!\n",
    "\n",
    "First we create the perspective Widget to visualize the streaming data later. The widget starts empty, but will start displaying data when `csp` runs the graph. \n",
    "\n",
    "(Note that if you are running this example locally on JupyterLab, you may need to restart the Jupyter server after installing the Perspective library in order to visualize the widget.)\n",
    "\n",
    "We group the results by the actual `bot` flag (true/false) and then by the predicted value (true/false) in order to form the confusion matrix. We also show the latest timestamp for each record so we can convince ourselves it's running on live events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c830ba-9223-4380-9e37-ac7b4b153294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b884aff722f413e90de3ecfe8655748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PerspectiveWidget(aggregates={'prediction': 'count', 'timestamp': 'last'}, binding_mode='client-server', column…"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = {\n",
    "    \"prediction\": \"boolean\",\n",
    "    \"bot\": \"boolean\",\n",
    "    \"user\": \"string\",\n",
    "    \"timestamp\": \"datetime\",\n",
    "    \"title\": \"string\",\n",
    "    \"domain\": \"string\",\n",
    "}\n",
    "# widget = PerspectiveWidget(schema, group_by=[\"bot\", \"prediction\"], columns=[\"prediction\", \"timestamp\"], aggregates={\"prediction\": \"count\", \"timestamp\": \"last\"}, binding_mode=\"client-server\")\n",
    "widget = PerspectiveWidget(\n",
    "    schema,\n",
    "    plugin=\"X Bar\",\n",
    "    group_by=[\"bot\", \"prediction\"],\n",
    "    columns=[\"prediction\"],\n",
    "    aggregates={\"prediction\": \"count\", \"timestamp\": \"last\"},\n",
    "    binding_mode=\"client-server\",\n",
    ")\n",
    "widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df19871-090d-4a76-be48-6cc31005677c",
   "metadata": {},
   "source": [
    "Next we run the graph against the previously trained model and the widget. Data should start appearing in the widget above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710cb50-3d2c-4160-ba43-54cc47561d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FetchWikiDataAdapter::start\n",
      "FetchWikiDataAdapter::stop\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start = utc_now()\n",
    "csp.run(inference_graph, clf, widget=widget, starttime=start, endtime=start + timedelta(seconds=30), realtime=True)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105712d-086b-4c4f-9e66-892e4473bca8",
   "metadata": {},
   "source": [
    "## Raw Data Exploration\n",
    "\n",
    "We can also use perspective to explore the raw historical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "324ffe2e-02df-4f04-b697-b6f04366f552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be484b0848248eea325f2759f2bcf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PerspectiveWidget(binding_mode='server', columns=['index', '$schema', 'id', 'type', 'namespace', 'title', 'tit…"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PerspectiveWidget(pd.json_normalize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659c125-e7c1-405d-9ef4-59afaf7dfdc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ccrt311-0.18.1]",
   "language": "python",
   "name": "conda-env-.conda-ccrt311-0.18.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
